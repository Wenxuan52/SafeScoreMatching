import argparse
import csv
import json
import os
from typing import Dict, List

os.environ.setdefault("MPLCONFIGDIR", "/tmp/matplotlib_config")
os.makedirs(os.environ["MPLCONFIGDIR"], exist_ok=True)

import matplotlib.pyplot as plt
import numpy as np

from jaxrl5.envs import make_env
from jaxrl5.tools import load_agent


def _reference_circle(num_points: int = 360):
    angles = np.linspace(0, 2 * np.pi, num_points, endpoint=False)
    x = np.cos(angles)
    z = 1.0 + np.sin(angles)
    return x, z


def _rescale_action_from_unit(action: np.ndarray, action_space) -> np.ndarray:
    low = np.asarray(action_space.low, dtype=np.float32)
    high = np.asarray(action_space.high, dtype=np.float32)
    if np.allclose(low, -1.0) and np.allclose(high, 1.0):
        return np.asarray(action, dtype=np.float32)

    scale = (high - low) / 2.0
    bias = (high + low) / 2.0
    return scale * np.asarray(action, dtype=np.float32) + bias


def rollout_episodes(
    env,
    policy_fn,
    episodes: int,
    deterministic: bool,
    seed: int,
    algo: str,
    guidance_mode: str,
    run_name: str,
):
    trajectories: List[List[Dict[str, float]]] = []
    for ep in range(episodes):
        obs, info = env.reset(
            seed=seed + ep,
            options={
                "init_x": 1.0,
                "init_z": 1.0,
                "init_vx": 0.0,
                "init_vz": 0.0,
                "init_theta": 0.0,
                "init_omega": 0.0,
                "init_waypoint_idx": 0,
            },
        )

        print("start x,z =", float(obs[0]), float(obs[2]))
        ep_data: List[Dict[str, float]] = []
        t = 0
        while True:
            policy_out = policy_fn(obs)
            if isinstance(policy_out, tuple) and len(policy_out) == 2:
                action, debug = policy_out
            else:
                action, debug = policy_out, {}

            action = np.clip(action, env.action_space.low, env.action_space.high)
            step_out = env.step(action)
            if len(step_out) == 6:
                next_obs, reward, cost, terminated, truncated, info = step_out
            else:
                next_obs, reward, terminated, truncated, info = step_out
                cost = float(info.get("cost", 0.0))
            x, z = float(obs[0]), float(obs[2])
            x_ref, z_ref = float(obs[6]), float(obs[8])
            idx = int(info.get("idx", t % 360))
            raw_action = debug.get("raw_action")
            env_action = debug.get("env_action", action)
            ep_data.append(
                {
                    "t": t,
                    "x": x,
                    "z": z,
                    "x_ref": x_ref,
                    "z_ref": z_ref,
                    "raw_action": raw_action.tolist() if raw_action is not None else np.asarray(env_action).tolist(),
                    "env_action": np.asarray(env_action).tolist(),
                    "reward": float(reward),
                    "cost": float(cost),
                    "h": float(info.get("h", 0.0)),
                    "terminated": bool(terminated),
                    "truncated": bool(truncated),
                    "idx": idx,
                    "guidance_mode": guidance_mode,
                    "run_name": run_name,
                }
            )
            obs = next_obs
            t += 1
            if terminated or truncated:
                break
        trajectories.append(ep_data)
    return trajectories


def plot_trajectories(trajectories: List[List[Dict[str, float]]], out_path: str, algo: str, checkpoint_label: str) -> None:
    plt.figure(figsize=(8, 8))
    plt.axhline(0.5, color="black", linestyle="-")
    plt.axhline(1.5, color="black", linestyle="-")
    circle_x, circle_z = _reference_circle()
    plt.plot(circle_x, circle_z, "k--", label="reference circle")

    colors = plt.cm.tab10.colors
    for i, ep_data in enumerate(trajectories):
        xs = [d["x"] for d in ep_data]
        zs = [d["z"] for d in ep_data]
        status = "term" if ep_data[-1]["terminated"] else ("trunc" if ep_data[-1]["truncated"] else "done")
        plt.plot(xs, zs, color=colors[i % len(colors)], label=f"episode {i} ({status})")

    plt.xlabel("x")
    plt.ylabel("z")
    plt.title(f"{algo.upper()} trajectory @ {checkpoint_label}")
    plt.axis("equal")
    plt.legend()
    plt.tight_layout()

    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    plt.savefig(out_path)
    plt.close()


def save_csv(trajectories: List[List[Dict[str, float]]], out_path: str) -> None:
    os.makedirs(os.path.dirname(out_path), exist_ok=True)
    fieldnames = [
        "episode",
        "t",
        "x",
        "z",
        "x_ref",
        "z_ref",
        "reward",
        "cost",
        "h",
        "terminated",
        "truncated",
        "idx",
    ]
    with open(out_path, "w", newline="", encoding="utf-8") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        for ep_idx, ep_data in enumerate(trajectories):
            for row in ep_data:
                writer.writerow({"episode": ep_idx, **row})


def main():
    parser = argparse.ArgumentParser(description="Visualize Quadrotor policy trajectories")
    parser.add_argument("--env_name", default="QuadrotorTracking2D-v0")
    parser.add_argument("--seed", type=int, default=0)
    parser.add_argument("--num_episodes", type=int, default=1)
    parser.add_argument("--episodes", type=int, default=None, help="Alias for num_episodes")
    parser.add_argument("--deterministic", action="store_true", help="Use deterministic actions (eval)")
    parser.add_argument("--algo", choices=["td3", "td3_lag", "ssm"], default=None)
    parser.add_argument("--agent", choices=["td3", "td3_lag", "ssm"], default=None, help="Deprecated alias for --algo")
    parser.add_argument("--ckpt_path", default=None, help="Checkpoint directory or file")
    parser.add_argument("--checkpoint_dir", default=None, help="Deprecated alias for --ckpt_path")
    parser.add_argument("--step", type=int, default=None, help="Checkpoint step (optional)")
    parser.add_argument("--checkpoint_step", type=int, default=None, help="Deprecated alias for --step")
    parser.add_argument("--out_dir", default="results/visualizations")
    parser.add_argument("--save_csv", action="store_true")
    parser.add_argument("--ssm_ddpm_temperature", type=float, default=None, help="Override SSM ddpm_temperature during evaluation")
    parser.add_argument("--config_path", default=None, help="Optional path to SSM config.json/variant.json")
    parser.add_argument(
        "--guidance_mode",
        choices=["none", "reward_only", "safety_only", "both"],
        default="both",
        help="Guidance mode for SSM/DDPM policy.",
    )
    parser.add_argument(
        "--run_name",
        default=None,
        help="Optional run name used for debug rollout json naming.",
    )
    args = parser.parse_args()

    algo = args.algo if args.algo is not None else (args.agent or "td3")
    ckpt_path = args.ckpt_path or args.checkpoint_dir
    if ckpt_path is None:
        parser.error("--ckpt_path (or --checkpoint_dir) is required")
    step = args.step if args.step is not None else args.checkpoint_step
    episodes = args.episodes if args.episodes is not None else args.num_episodes

    env = make_env(args.env_name, seed=args.seed)

    agent, base_policy_fn, meta = load_agent(
        algo,
        ckpt_path,
        step=step,
        observation_space=env.observation_space,
        action_space=env.action_space,
        seed=args.seed,
        deterministic=args.deterministic,
        ddpm_temperature=args.ssm_ddpm_temperature,
        config_path=args.config_path,
        guidance_mode=args.guidance_mode,
    )
    resolved_ckpt = meta.get("ckpt_resolved_path", ckpt_path)

    ssm_state = {"agent": agent}

    def policy_fn(obs: np.ndarray):
        if algo != "ssm":
            env_action = base_policy_fn(obs)
            return env_action, {"raw_action": np.asarray(env_action)}

        raw_action, new_agent = ssm_state["agent"].eval_actions(
            np.asarray(obs, dtype=np.float32), guidance_mode=args.guidance_mode
        )
        ssm_state["agent"] = new_agent
        env_action = _rescale_action_from_unit(raw_action, env.action_space)
        env_action = np.clip(env_action, env.action_space.low, env.action_space.high)
        return env_action, {
            "raw_action": np.asarray(raw_action, dtype=np.float32),
            "env_action": np.asarray(env_action, dtype=np.float32),
        }

    trajectories = rollout_episodes(
        env,
        policy_fn,
        episodes=episodes,
        deterministic=args.deterministic,
        seed=args.seed,
        algo=algo,
        guidance_mode=args.guidance_mode,
        run_name=args.run_name or os.path.basename(resolved_ckpt),
    )

    ckpt_label = os.path.basename(resolved_ckpt)
    out_png = os.path.join(
        args.out_dir, f"traj_{algo}_{ckpt_label}_guidance-{args.guidance_mode}.png"
    )
    plot_trajectories(trajectories, out_png, algo, ckpt_label)
    print(f"Saved trajectory plot to {out_png}")

    debug_dir = os.path.join("results", "debug_rollouts")
    os.makedirs(debug_dir, exist_ok=True)
    debug_path = os.path.join(
        debug_dir,
        f"{(args.run_name or ckpt_label)}_{args.guidance_mode}.json",
    )
    with open(debug_path, "w", encoding="utf-8") as f:
        first_episode = trajectories[0] if trajectories else []
        json_payload = {
            "guidance_mode": args.guidance_mode,
            "run_name": args.run_name or ckpt_label,
            "steps": first_episode[:20],
        }
        json.dump(json_payload, f, indent=2)
    print(f"Saved debug rollout (first 20 steps) to {debug_path}")

    if args.save_csv:
        out_csv = os.path.join(
            args.out_dir, f"traj_{algo}_{ckpt_label}_guidance-{args.guidance_mode}.csv"
        )
        save_csv(trajectories, out_csv)
        print(f"Saved trajectory CSV to {out_csv}")

    env.close()


if __name__ == "__main__":
    main()
